use anyhow::{anyhow, Result};
use futures_util::FutureExt;
use serde::{Deserialize, Serialize};
use serenity::{
    model::{
        channel::{AttachmentType, Channel, GuildChannel},
        gateway::GatewayIntents,
    },
    Client,
};
use std::{borrow::Cow, io::Write, sync::Arc};
use tokio::sync::Mutex;

use crate::config::Config;

pub struct DiscordDrive {
    channel: GuildChannel,
    http: Arc<serenity::http::client::Http>,
}

impl DiscordDrive {
    const MAX_FILE_SIZE: usize = 1_000_000;
    pub async fn new(config: Config) -> Result<Self> {
        let intents = GatewayIntents::GUILD_MESSAGES | GatewayIntents::MESSAGE_CONTENT;

        let client = Client::builder(&config.token, intents).await?;
        let Channel::Guild(channel) = client
            .cache_and_http
            .http
            .get_channel(config.channel)
            .await?
        else {
            panic!("Only guild channels are supported at the moment :(") // TODO: return Err
        };
        Ok(Self {
            http: client.cache_and_http.http.clone(),
            channel,
        })
    }
    async fn upload_bytes(&mut self, bytes: &[u8]) -> Result<Vec<u64>> {
        let file_chunks = bytes.chunks(Self::MAX_FILE_SIZE);
        let attachments = file_chunks.enumerate().map(|(i, c)| AttachmentType::Bytes {
            data: Cow::from(c),
            filename: format!("{i}.txt"),
        });
        let ids = Arc::new(Mutex::new(Vec::with_capacity(attachments.len())));
        for _ in 0..attachments.len() {
            ids.lock().await.push(0);
        }
        futures::future::try_join_all(attachments.enumerate().map(|(i, a)| {
            let ids2 = ids.clone();
            let i = Arc::new(Mutex::new(i));
            self.channel
                .send_files(&self.http, vec![a], |m| m.content(""))
                .then(|m| async move {
                    match m {
                        Ok(m) => {
                            ids2.lock().await[Arc::try_unwrap(i).unwrap().into_inner()] = m.id.0;
                            Ok(())
                        }
                        Err(e) => Err(e),
                    }
                })
        }))
        .await?;
        let ids = Arc::try_unwrap(ids).unwrap();
        let ids = ids.into_inner();
        Ok(ids)
    }

    async fn download_bytes(&mut self, ids: &[u64]) -> Result<Vec<u8>> {
        let messages = ids
            .iter()
            .map(|id| self.http.get_message(self.channel.id.0, *id))
            .collect::<Vec<_>>();

        let messages = futures::future::try_join_all(messages).await?;
        let mut attachments = Vec::with_capacity(messages.len());
        for message in messages {
            if message.attachments.len() != 1 {
                return Err(anyhow!("message didn't have a singular attachment"));
            }
            attachments.push(message.attachments[0].clone());
        }

        let attachments =
            futures::future::try_join_all(attachments.iter().map(|a| a.download())).await?;

        let output = attachments.iter().flatten().cloned().collect::<Vec<_>>();

        Ok(output)
    }
}

#[derive(Serialize, Deserialize, Clone)]
pub struct ChunkManager {
    chunk_table: Vec<Option<u64>>,
}

impl ChunkManager {
    pub fn from_file_or_new(new_size: usize) -> Result<ChunkManager> {
        let path = Self::get_file_path();
        if std::path::Path::exists(&path) {
            let content = std::fs::read_to_string(&path)?;
            return Ok(serde_json::from_str(&content)?);
        }

        let mut chunk_table = Vec::with_capacity(new_size);
        for _ in 0..new_size {
            chunk_table.push(None);
        }
        Ok(Self { chunk_table })
    }

    fn save_to_file(&self) -> Result<()> {
        let path = Self::get_file_path();

        let mut file = std::fs::OpenOptions::new()
            .write(true)
            .create(true)
            .open(path)?;
        file.write_all(serde_json::to_string(&self)?.as_bytes())?;
        Ok(())
    }
    fn get_file_path() -> std::path::PathBuf {
        "/etc/discorhdd/chunks.json".into()
    }

    fn get_chunk_indexes(&self, start: usize, len: usize) -> Result<Vec<usize>> {
        let start_idx = start / DiscordDrive::MAX_FILE_SIZE;
        let end_idx = (start + len) / DiscordDrive::MAX_FILE_SIZE;

        Ok((start_idx..=end_idx).collect())
    }

    async fn read_entire_chunks(
        &self,
        start: usize,
        len: usize,
        drive: &mut DiscordDrive,
    ) -> Result<Vec<u8>> {
        let chunk_ids = self
            .get_chunk_indexes(start, len)?
            .iter()
            .map(|x| self.chunk_table[*x])
            .collect::<Vec<_>>();
        let mut buf = Vec::with_capacity(len);
        for chunk_idx in chunk_ids {
            match chunk_idx {
                Some(id) => {
                    let mut down = drive.download_bytes(&[id]).await?;
                    while down.len() != DiscordDrive::MAX_FILE_SIZE {
                        down.push(0);
                    }
                    buf.append(&mut down);
                }
                None => {
                    buf.append(&mut vec![0; DiscordDrive::MAX_FILE_SIZE]);
                }
            }
        }
        Ok(buf)
    }

    pub async fn read(
        &self,
        start: usize,
        len: usize,
        drive: &mut DiscordDrive,
    ) -> Result<Vec<u8>> {
        let buf = self.read_entire_chunks(start, len, drive).await?;
        Ok(buf
            [(start % DiscordDrive::MAX_FILE_SIZE)..((start % DiscordDrive::MAX_FILE_SIZE) + len)]
            .to_vec())
    }
    pub async fn write(
        &mut self,
        start: usize,
        data: &[u8],
        drive: &mut DiscordDrive,
    ) -> Result<()> {
        let len = data.len();
        let mut old_data = self.read_entire_chunks(start, len, drive).await?;

        for (read_idx, data_idx) in ((start % DiscordDrive::MAX_FILE_SIZE)
            ..((start % DiscordDrive::MAX_FILE_SIZE) + len))
            .enumerate()
        {
            old_data[data_idx] = data[read_idx];
        }

        let new_data_chunks = old_data
            .chunks(DiscordDrive::MAX_FILE_SIZE)
            .collect::<Vec<_>>();

        let chunk_idxs = self.get_chunk_indexes(start, len)?;

        if chunk_idxs.len() != new_data_chunks.len() {
            return Err(anyhow!("chunk mismatch"));
        }

        for (chunk_data, chunk_idx) in new_data_chunks.iter().zip(chunk_idxs) {
            let message_ids = drive.upload_bytes(chunk_data).await?;
            if message_ids.len() != 1 {
                return Err(anyhow!("currently chunk size should match, but it doesn't"));
            }
            let message_id = message_ids[0];
            self.chunk_table[chunk_idx] = Some(message_id);
        }

        self.save_to_file()?;
        Ok(())
    }
}

impl Drop for ChunkManager {
    fn drop(&mut self) {
        self.save_to_file()
            .unwrap_or_else(|e| println!("failed to save chunks to disk: {e}"));
    }
}
